{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” Azure Document Extraction Pipeline\n",
    "## Using Responses API with Structured Outputs\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "ğŸ“„ Document (PDF/Image)\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Mistral Document AI     â”‚  â† OCR + Text Extraction\n",
    "â”‚  (mistral-ocr-2503)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼ OCR Text + Original Image\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GPT-5.1                 â”‚  â† Structured Extraction (Responses API)\n",
    "â”‚  (Responses API +        â”‚     with strict JSON Schema\n",
    "â”‚   Structured Outputs)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼ Extraction Results\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Azure AI Evaluation SDK â”‚  â† Quality Evaluation\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Install Dependencies\n",
    "\n",
    "Run the cell below to install required packages. Key libraries:\n",
    "- `openai` - Azure OpenAI client for GPT models\n",
    "- `azure-ai-evaluation` - Quality metrics (Groundedness, Relevance, Coherence)\n",
    "- `azure-identity` - Entra ID authentication\n",
    "- `pydantic` - Schema definition for Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade openai azure-ai-evaluation azure-identity python-dotenv pydantic requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Configuration\n",
    "\n",
    "Create a `.env` file in the same directory with the following variables:\n",
    "\n",
    "```env\n",
    "# Mistral Document AI\n",
    "MISTRAL_ENDPOINT=https://<your-resource>.services.ai.azure.com\n",
    "MISTRAL_API_KEY=<your-api-key>\n",
    "MISTRAL_MODEL=mistral-ocr-2503\n",
    "\n",
    "# Azure OpenAI (Responses API)\n",
    "# Found in: Foundry > Models + endpoints > Your deployment\n",
    "AZURE_OPENAI_ENDPOINT=https://<your-resource>.cognitiveservices.azure.com\n",
    "AZURE_OPENAI_API_KEY=<your-api-key>\n",
    "AZURE_OPENAI_DEPLOYMENT=gpt-5.1\n",
    "\n",
    "# Evaluation Model\n",
    "EVAL_MODEL_DEPLOYMENT=gpt-4o\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules imported successfully\n",
      "   - Config, Utils, OCR, Extractor, Evaluator, Confidence, Schemas, Pipeline\n"
     ]
    }
   ],
   "source": [
    "# Add src/ to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "# Import from our modules\n",
    "from config import Config\n",
    "from utils import get_file_info, encode_file_to_base64, is_pdf\n",
    "from ocr import MistralOCR, create_ocr_client\n",
    "from extractor import StructuredExtractor, create_extractor\n",
    "from evaluator import QualityEvaluator, ExtractionValidator, create_evaluator\n",
    "from confidence import ConfidenceCalculator, create_confidence_calculator  # NEW: OCR vs Extraction comparison\n",
    "from schemas import DocumentExtraction, EXTRACTION_SCHEMA, get_strict_schema\n",
    "from pipeline import DocumentPipeline\n",
    "\n",
    "# Standard imports for notebook usage\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "print(\"âœ… All modules imported successfully\")\n",
    "print(\"   - Config, Utils, OCR, Extractor, Evaluator, Confidence, Schemas, Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Configuration Status\n",
      "==================================================\n",
      "âœ… Mistral Document AI\n",
      "   Endpoint: https://felat-mgtjiioy-francecentral.cognitiveserv...\n",
      "   Model: mistral-document-ai-2505-2\n",
      "\n",
      "âœ… Azure OpenAI (Responses API)\n",
      "   Endpoint: https://doc-processing-mistral-gpt-eval.cognitives...\n",
      "   Base URL: https://doc-processing-mistral-gpt-eval.cognitiveservices.azure.com/openai/v1/\n",
      "   Deployment: gpt-5.1\n",
      "\n",
      "âœ… Evaluation: Configured\n",
      "   Deployment: gpt-4o\n",
      "==================================================\n",
      "\n",
      "ğŸ“‹ Extraction mode: hybrid\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configuration - Load from .env file\n",
    "# =============================================================================\n",
    "\n",
    "# Load configuration from environment variables\n",
    "config = Config.from_env()\n",
    "\n",
    "# Display configuration status\n",
    "status = config.show_status()\n",
    "\n",
    "# Verify all required services are configured\n",
    "if not status[\"mistral_configured\"]:\n",
    "    print(\"\\nâš ï¸ Warning: Mistral OCR not configured - check MISTRAL_ENDPOINT and MISTRAL_API_KEY\")\n",
    "if not status[\"aoai_configured\"]:\n",
    "    print(\"\\nâš ï¸ Warning: Azure OpenAI not configured - check AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Extraction mode: {config.extraction_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Define Your Extraction Schema\n",
    "\n",
    "We use **Pydantic models** to define what data we want to extract. Azure OpenAI's **Structured Outputs** feature guarantees the model returns valid JSON matching our schema exactly.\n",
    "\n",
    "The schema is converted to \"strict mode\" which requires `additionalProperties: false` and all fields in `required`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Extraction Schema (Strict Mode for Azure OpenAI):\n",
      "   Schema name: DocumentExtraction\n",
      "   Fields: ['document_type', 'document_number', 'document_date', 'supplier', 'customer', 'total_amount', 'line_items']\n",
      "\n",
      "   ğŸ’¡ To see full schema: print(json.dumps(EXTRACTION_SCHEMA, indent=2))\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Extraction Schema - Already imported from src/schemas.py\n",
    "# =============================================================================\n",
    "\n",
    "# The schema is imported from src/schemas.py\n",
    "# You can view the Pydantic models: DocumentExtraction, LineItem, MonetaryAmount, PersonInfo\n",
    "\n",
    "print(\"ğŸ“‹ Extraction Schema (Strict Mode for Azure OpenAI):\")\n",
    "print(f\"   Schema name: DocumentExtraction\")\n",
    "print(f\"   Fields: {list(EXTRACTION_SCHEMA.get('properties', {}).keys())}\")\n",
    "print(\"\\n   ğŸ’¡ To see full schema: print(json.dumps(EXTRACTION_SCHEMA, indent=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Document Utilities\n",
    "\n",
    "Helper functions to handle document files: encode to base64, detect MIME types, and check file properties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document utilities ready\n",
      "   Test file: invoice.png (0.353 MB)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Document Utilities - Already imported from src/utils.py\n",
    "# =============================================================================\n",
    "\n",
    "# Functions available:\n",
    "# - encode_file_to_base64(file_path) -> (base64_string, mime_type)\n",
    "# - get_file_info(file_path) -> dict with name, extension, size\n",
    "# - is_pdf(file_path) -> bool\n",
    "\n",
    "# Test with a sample file\n",
    "test_file = \"invoice.png\"\n",
    "if Path(test_file).exists():\n",
    "    info = get_file_info(test_file)\n",
    "    print(f\"âœ… Document utilities ready\")\n",
    "    print(f\"   Test file: {info['name']} ({info['size_mb']} MB)\")\n",
    "else:\n",
    "    print(\"âœ… Document utilities ready\")\n",
    "    print(\"   â„¹ï¸ Place a document (PDF/image) to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Step 1: OCR with Mistral Document AI\n",
    "\n",
    "**Mistral Document AI** is a specialized OCR model that extracts text from documents (PDFs, images) and returns it in **Markdown format**.\n",
    "\n",
    "This is powerful because:\n",
    "- Tables are preserved as Markdown tables\n",
    "- Headers and structure are maintained\n",
    "- The output is clean and LLM-ready\n",
    "\n",
    "The OCR text becomes the **context** for our extraction model and serves as **ground truth** for confidence calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mistral OCR client initialized\n",
      "   Model: mistral-document-ai-2505-2\n",
      "   Endpoint: https://felat-mgtjiioy-francecentral.cognitiveserv...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OCR Client - Using src/ocr.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create OCR client from configuration\n",
    "ocr_client = create_ocr_client(config)\n",
    "\n",
    "print(f\"âœ… Mistral OCR client initialized\")\n",
    "print(f\"   Model: {config.mistral_model}\")\n",
    "print(f\"   Endpoint: {config.mistral_endpoint[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Step 2: Structured Extraction with GPT-5.1\n",
    "\n",
    "GPT-5.1 extracts structured data from the OCR text using our Pydantic schema.\n",
    "\n",
    "**Two extraction modes available:**\n",
    "- `text_only` - Uses only OCR text (faster, cheaper)\n",
    "- `hybrid` - Sends both OCR text AND the original image (more accurate for complex layouts)\n",
    "\n",
    "The model returns JSON that exactly matches our schema - no parsing errors possible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… StructuredExtractor initialized with Entra ID\n",
      "   Deployment: gpt-5.1\n",
      "   Extraction mode: hybrid\n",
      "   Available methods:\n",
      "     - extract(text, schema)           # Text only\n",
      "     - extract_hybrid(ocr_text, image) # OCR + Image (recommended)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Structured Extractor - Using src/extractor.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create extractor from configuration (uses Entra ID auth)\n",
    "extractor = create_extractor(config)\n",
    "\n",
    "print(f\"âœ… StructuredExtractor initialized with Entra ID\")\n",
    "print(f\"   Deployment: {config.aoai_deployment}\")\n",
    "print(f\"   Extraction mode: {config.extraction_mode}\")\n",
    "print(f\"   Available methods:\")\n",
    "print(f\"     - extract(text, schema)           # Text only\")\n",
    "print(f\"     - extract_hybrid(ocr_text, image) # OCR + Image (recommended)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Step 3: Quality Evaluation\n",
    "\n",
    "This is the **core of our pipeline**. We evaluate extraction quality using two complementary approaches:\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Approach 1: Confidence Score (OCR vs Extraction) â€” No LLM!\n",
    "\n",
    "We **compare extracted values against the original OCR text** using pure text matching (no AI).\n",
    "\n",
    "#### How it works (Ctrl+F approach)\n",
    "\n",
    "```\n",
    "GPT extracts: { \"supplier_address\": \"123 Main Street\" }\n",
    "\n",
    "                    â”‚\n",
    "                    â–¼\n",
    "\n",
    "    Is \"123 Main Street\" somewhere in the OCR text?\n",
    "    (Like doing Ctrl+F in a Word document)\n",
    "\n",
    "                    â”‚\n",
    "        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "        â”‚                       â”‚\n",
    "        â–¼                       â–¼\n",
    "    FOUND                   NOT FOUND\n",
    "    Confidence = 1.0 âœ…      Confidence = 0.0 âŒ\n",
    "                            (Hallucination!)\n",
    "```\n",
    "\n",
    "#### Fuzzy Matching (tolerates typos)\n",
    "\n",
    "What if there's a small difference? We use **fuzzy matching** (`difflib.SequenceMatcher`):\n",
    "\n",
    "```python\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "# Compare two strings\n",
    "ratio = SequenceMatcher(None, \"Chan Accounting\", \"Chan Acounting\").ratio()\n",
    "# ratio = 0.96 (96% similar)\n",
    "\n",
    "if ratio >= 0.7:  # Our threshold\n",
    "    confidence = ratio  # âš ï¸ Fuzzy match (keeps actual ratio!)\n",
    "else:\n",
    "    confidence = 0.0    # âŒ Not found\n",
    "```\n",
    "\n",
    "| OCR Text | GPT Extracted | Similarity | Result |\n",
    "|----------|---------------|------------|--------|\n",
    "| \"Chan Accounting\" | \"Chan Accounting\" | 100% | âœ… 1.0 (exact) |\n",
    "| \"Chan Accounting\" | \"Chan Acounting\" | 96% | âš ï¸ 0.96 (fuzzy) |\n",
    "| \"Chan Accounting\" | \"Chan Acount\" | 85% | âš ï¸ 0.85 (fuzzy) |\n",
    "| \"Chan Accounting\" | \"John Smith\" | 20% | âŒ 0.0 (hallucination) |\n",
    "\n",
    "> âš ï¸ **Important**: The 70% threshold determines if it's a match or not, but the **confidence score = the actual similarity ratio**. So 70% match â†’ score of 0.70, not 1.0. This penalizes fuzzy matches proportionally.\n",
    "\n",
    "> ğŸ’¡ **Key insight**: This is **deterministic** (same input = same output), **free** (no API calls), and **fast**. But it only checks if the text exists, not if it's semantically correct.\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Approach 2: Azure AI Evaluation SDK (LLM-as-a-Judge)\n",
    "\n",
    "These are **AI-assisted evaluators** that use **GPT-4o as a judge** to assess quality:\n",
    "\n",
    "| Metric | What it measures | Why it matters |\n",
    "|--------|------------------|----------------|\n",
    "| **Groundedness** | Is the extraction supported by the source text? | Detects hallucinations semantically |\n",
    "| **Relevance** | Does the extraction answer the query? | Ensures we extracted the RIGHT fields |\n",
    "| **Coherence** | Is the output logically structured? | Validates JSON quality and readability |\n",
    "\n",
    "#### How LLM-as-a-Judge works\n",
    "\n",
    "The SDK sends to the judge model (GPT-4o):\n",
    "1. **Prompt template** with metric definition and scoring rubric (Likert scale 1-5)\n",
    "2. **Context** = OCR text (grounding source)\n",
    "3. **Response** = Extracted JSON\n",
    "4. **Query** = Our extraction instruction\n",
    "\n",
    "The judge model returns:\n",
    "- A **score** (1-5) for each metric\n",
    "- A **reason** explaining why (useful for debugging)\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    GPT-4o (Judge)                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Input:                                                 â”‚\n",
    "â”‚  - Prompt: \"Evaluate groundedness on scale 1-5...\"      â”‚\n",
    "â”‚  - Context: \"Invoice #0324, Date: 2024-01-15...\"        â”‚\n",
    "â”‚  - Response: {\"document_number\": \"0324\", ...}           â”‚\n",
    "â”‚  - Query: \"Extract document information\"                â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚  Output:                                                â”‚\n",
    "â”‚  - groundedness: 5                                      â”‚\n",
    "â”‚  - groundedness_reason: \"All extracted values are       â”‚\n",
    "â”‚    directly supported by the source text...\"            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ How Both Approaches Work Together\n",
    "\n",
    "The two approaches are **independent and complementary**. They run in parallel and answer different questions:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                      PIPELINE FLOW                              â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                 â”‚\n",
    "â”‚  OCR Text â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚\n",
    "â”‚                    â”‚                                            â”‚\n",
    "â”‚  Extracted JSON â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ â”‚\n",
    "â”‚                    â”‚                                            â”‚\n",
    "â”‚                    â–¼                                            â”‚\n",
    "â”‚         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚\n",
    "â”‚         â”‚  Approach 1:        â”‚    â”‚  Approach 2:            â”‚  â”‚\n",
    "â”‚         â”‚  Fuzzy Matching     â”‚    â”‚  LLM-as-a-Judge         â”‚  â”‚\n",
    "â”‚         â”‚  (per field)        â”‚    â”‚  (whole extraction)     â”‚  â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚\n",
    "â”‚                    â”‚                           â”‚                â”‚\n",
    "â”‚                    â–¼                           â–¼                â”‚\n",
    "â”‚         confidence: 0.92           groundedness: 5              â”‚\n",
    "â”‚         (field-level scores)       relevance: 5                 â”‚\n",
    "â”‚                                    coherence: 4                 â”‚\n",
    "â”‚                                                                 â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### What each metric tells you\n",
    "\n",
    "| Question | Fuzzy | Groundedness | Relevance | Coherence |\n",
    "|----------|-------|--------------|-----------|-----------|\n",
    "| \"Is the text physically present in OCR?\" | âœ… | âŒ | âŒ | âŒ |\n",
    "| \"Is the meaning correct (even if reformatted)?\" | âŒ | âœ… | âŒ | âŒ |\n",
    "| \"Did we extract the RIGHT field?\" | âŒ | âŒ | âœ… | âŒ |\n",
    "| \"Is the JSON well-structured?\" | âŒ | âŒ | âŒ | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“Š Interpretation Matrix: How to Read Combined Results\n",
    "\n",
    "Use this matrix to diagnose extraction quality:\n",
    "\n",
    "| Fuzzy | Ground. | Relev. | Coher. | Diagnosis | Action |\n",
    "|-------|---------|--------|--------|-----------|--------|\n",
    "| **1.0** | **5** | **5** | **5** | âœ… **Perfect** â€” Exact text, correct meaning, right field, clean JSON | Trust the result |\n",
    "| **0.7-0.99** | **5** | **5** | **5** | âš ï¸ **Minor OCR issue** â€” Small typo but everything else correct | Acceptable |\n",
    "| **0.0** | **5** | **5** | **5** | ğŸ”„ **Reformatted** â€” Different format but semantically correct | Acceptable |\n",
    "| **1.0** | **5** | **1-2** | **5** | ğŸ¯ **Wrong field** â€” Text exists but assigned to wrong field | Review schema/prompt |\n",
    "| **1.0** | **5** | **5** | **1-2** | ğŸ“ **Bad JSON** â€” Values OK but structure issues | Check schema |\n",
    "| **0.0** | **1-2** | **?** | **?** | âŒ **Hallucination** â€” GPT invented this value | Reject |\n",
    "\n",
    "#### Concrete example\n",
    "\n",
    "```\n",
    "Document OCR: \"Invoice Date: 15/01/2024, Delivery Date: 20/01/2024\"\n",
    "\n",
    "Extraction: { \"invoice_date\": \"20/01/2024\" }  â† ERROR: wrong date!\n",
    "\n",
    "Scores:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric         â”‚ Score â”‚ Why                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Fuzzy          â”‚ 1.0 âœ…â”‚ \"20/01/2024\" exists in OCR text         â”‚\n",
    "â”‚ Groundedness   â”‚ 5 âœ…  â”‚ Value comes from the document           â”‚\n",
    "â”‚ Relevance      â”‚ 1 âŒ  â”‚ Wrong date! (delivery vs invoice)       â”‚\n",
    "â”‚ Coherence      â”‚ 5 âœ…  â”‚ JSON is well-formed                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Diagnosis: Relevance catches the error that Fuzzy + Groundedness missed!\n",
    "```\n",
    "\n",
    "#### Another example: Date reformatting\n",
    "\n",
    "```\n",
    "Document OCR: \"Date: 15/01/2024\"\n",
    "\n",
    "Extraction: { \"invoice_date\": \"January 15, 2024\" }  â† Same date, different format\n",
    "\n",
    "Scores:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Metric         â”‚ Score â”‚ Why                                     â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Fuzzy          â”‚ 0.0 âŒâ”‚ \"January 15, 2024\" â‰  \"15/01/2024\"       â”‚\n",
    "â”‚ Groundedness   â”‚ 5 âœ…  â”‚ Same date, just reformatted             â”‚\n",
    "â”‚ Relevance      â”‚ 5 âœ…  â”‚ Correct field                           â”‚\n",
    "â”‚ Coherence      â”‚ 5 âœ…  â”‚ JSON is well-formed                     â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "Diagnosis: Fuzzy fails but Groundedness confirms it's correct â†’ Acceptable!\n",
    "```\n",
    "\n",
    "> ğŸ’¡ **Key takeaway**: No single metric tells the whole story. Use them together:\n",
    "> - **Fuzzy** = Fast/free sanity check\n",
    "> - **Groundedness** = Semantic correctness\n",
    "> - **Relevance** = Right field assignment\n",
    "> - **Coherence** = Output quality\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ”„ Comparison: Fuzzy Matching vs LLM-as-a-Judge\n",
    "\n",
    "| Aspect | Fuzzy Matching (Approach 1) | LLM-as-a-Judge (Approach 2) |\n",
    "|--------|-----------------------------|-----------------------------|\n",
    "| **Cost** | Free (pure Python) | Paid (API call to GPT-4o) |\n",
    "| **Speed** | Instant | ~1-2 seconds per evaluation |\n",
    "| **Intelligence** | Text matching only | Semantic understanding |\n",
    "| **Determinism** | Same input = same output | May vary slightly |\n",
    "| **Use case** | Quick hallucination check | Detailed quality assessment |\n",
    "\n",
    "> âš ï¸ **Important**: LLM-as-a-Judge requires a **separate model deployment** (GPT-4o). Configure `EVAL_MODEL_DEPLOYMENT=gpt-4o` in your `.env` file.\n",
    "\n",
    "> ğŸ“š **Documentation**: [RAG Evaluators](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/rag-evaluators) | [General Purpose Evaluators](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-evaluators/general-purpose-evaluators)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extraction validator initialized\n",
      "   Validates: dates, amounts\n",
      "\n",
      "âœ… Confidence Calculator initialized\n",
      "   Method: OCR vs Extraction text comparison\n",
      "   No more self-evaluation from GPT!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Extraction Validator & Confidence Calculator - Using src/evaluator.py & src/confidence.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create validator instance (dates, amounts)\n",
    "validator = ExtractionValidator()\n",
    "\n",
    "# Create confidence calculator (OCR vs Extraction comparison)\n",
    "# fuzzy_threshold=0.7 means: 70% text similarity = considered a match\n",
    "# Lower = more permissive (tolerates OCR errors), Higher = stricter\n",
    "confidence_calc = create_confidence_calculator(fuzzy_threshold=0.7)\n",
    "\n",
    "print(\"âœ… Extraction validator initialized\")\n",
    "print(\"   Validates: dates, amounts\")\n",
    "print(\"\\nâœ… Confidence Calculator initialized\")\n",
    "print(\"   Method: Text matching (Ctrl+F) + Fuzzy matching (difflib)\")\n",
    "print(\"   Threshold: 0.7 (70% similarity = match)\")\n",
    "print(\"   ğŸ’¡ No LLM used - pure Python, deterministic, free!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QualityEvaluator initialized\n",
      "   Judge model: gpt-4o\n",
      "   Metrics: Groundedness, Relevance, Coherence\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Quality Evaluator - Using src/evaluator.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create evaluator from configuration (uses GPT-4o as judge)\n",
    "if config.eval_deployment:\n",
    "    evaluator = create_evaluator(config)\n",
    "    print(f\"âœ… QualityEvaluator initialized\")\n",
    "    print(f\"   Judge model: {config.eval_deployment}\")\n",
    "    print(f\"   Metrics: Groundedness, Relevance, Coherence\")\n",
    "else:\n",
    "    evaluator = None\n",
    "    print(\"âš ï¸ Evaluator not configured (EVAL_MODEL_DEPLOYMENT not set)\")\n",
    "    print(\"   Set EVAL_MODEL_DEPLOYMENT=gpt-4o in .env to enable evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Complete Pipeline\n",
    "\n",
    "The `DocumentPipeline` class orchestrates all components: OCR â†’ Extraction â†’ Confidence â†’ Evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing pipeline components...\n",
      "   âœ… OCR client ready (model: mistral-document-ai-2505-2)\n",
      "   âœ… Extractor ready (model: gpt-5.1)\n",
      "   âœ… Evaluator ready (model: gpt-4o)\n",
      "   âœ… Confidence calculator ready (OCR vs Extraction comparison)\n",
      "âœ… Pipeline ready\n",
      "\n",
      "\n",
      "ğŸ“ Usage:\n",
      "   results = pipeline.process(\"document.pdf\")\n",
      "   results = pipeline.process(\"document.pdf\", run_evaluation=True)\n",
      "   pipeline.display_results(results)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Complete Pipeline - Using src/pipeline.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create pipeline from configuration\n",
    "pipeline = DocumentPipeline(config)\n",
    "\n",
    "print(\"\\nğŸ“ Usage:\")\n",
    "print('   results = pipeline.process(\"document.pdf\")')\n",
    "print('   results = pipeline.process(\"document.pdf\", run_evaluation=True)')\n",
    "print('   pipeline.display_results(results)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Example Usage\n",
    "\n",
    "Let's process a real document and see all components in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Display the document we're about to process\n",
    "# =============================================================================\n",
    "from IPython.display import Image, display\n",
    "\n",
    "document_path = \"invoice.png\"\n",
    "\n",
    "if Path(document_path).exists():\n",
    "    print(f\"ğŸ“„ Document: {document_path}\")\n",
    "    print(f\"   Size: {get_file_info(document_path)['size_mb']} MB\")\n",
    "    print(\"\\nğŸ‘‡ Preview:\")\n",
    "    display(Image(filename=document_path, width=600))\n",
    "else:\n",
    "    print(f\"âš ï¸ Document not found: {document_path}\")\n",
    "    print(\"   Place an invoice.png file in the project root to test.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing: invoice.png\n",
      "   Size: 0.353 MB\n",
      "\n",
      "ğŸ”„ Step 1: OCR with Mistral Document AI\n",
      "   âœ… OCR complete (1 pages)\n",
      "\n",
      "ğŸ”„ Step 2: Hybrid extraction with GPT-5.1 (OCR + Image)\n",
      "   âœ… Extraction complete (mode: hybrid)\n",
      "   ğŸ“Š Confidence (OCR vs Extraction): 87.0%\n",
      "\n",
      "ğŸ”„ Step 3: Quality evaluation\n",
      "   âœ… Evaluation complete (score: 77.8%)\n",
      "\n",
      "âœ… Processing complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Process a document\n",
    "# =============================================================================\n",
    "\n",
    "# Process the sample invoice\n",
    "results = pipeline.process(\n",
    "    file_path=\"invoice.png\",\n",
    "    run_evaluation=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š EXTRACTION RESULTS\n",
      "============================================================\n",
      "\n",
      "Document Type: invoice\n",
      "Document Number: 0324\n",
      "Document Date: None\n",
      "Total Amount: 500.0 USD\n",
      "\n",
      "ğŸ“Š Confidence Score: 87.0% (method: ocr_extraction_comparison)\n",
      "   âœ… High confidence fields: 5\n",
      "   âš ï¸  Medium confidence: 0\n",
      "   âŒ Low confidence: 1\n",
      "\n",
      "Line Items (5):\n",
      "   1. Sed ut perspiciatis unde omnis iste lore... â†’ 100.0\n",
      "   2. Adipisc ing elit, sed do eius mod tempor... â†’ 100.0\n",
      "   3. Nemo enim ipsam volupt atem quia volupta... â†’ 100.0\n",
      "   4. Sed ut perspiciatis unde omnis iste lore... â†’ 100.0\n",
      "   5. Adipisc ing elit, sed do eius mod tempor... â†’ 100.0\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ EVALUATION RESULTS\n",
      "============================================================\n",
      "   âš ï¸ Groundedness: 2.0/5\n",
      "   âœ… Relevance: 5.0/5\n",
      "   âœ… Coherence: 4.0/5\n",
      "\n",
      "Validation: 2/2 checks passed\n",
      "\n",
      "âš ï¸ OVERALL SCORE: 77.8%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Display Results\n",
    "# =============================================================================\n",
    "\n",
    "pipeline.display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” MÃ©tadonnÃ©es de l'extraction:\n",
      "   Mode: hybrid_ocr_vision\n",
      "   Input tokens: 1608\n",
      "   Output tokens: 260\n",
      "\n",
      "âœ… L'image a bien Ã©tÃ© envoyÃ©e avec le texte OCR\n"
     ]
    }
   ],
   "source": [
    "# Check extraction mode used\n",
    "metadata = results.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "print(\"ğŸ” Extraction Metadata:\")\n",
    "print(f\"   Mode: {metadata.get('mode', 'NOT DEFINED - probably text_only')}\")\n",
    "print(f\"   Input tokens: {metadata.get('usage', {}).get('input_tokens', 'N/A')}\")\n",
    "print(f\"   Output tokens: {metadata.get('usage', {}).get('output_tokens', 'N/A')}\")\n",
    "\n",
    "# If mode is not \"hybrid_ocr_vision\", the image was not sent\n",
    "if metadata.get('mode') == 'hybrid_ocr_vision':\n",
    "    print(\"\\nâœ… Image was sent along with OCR text\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Text-only mode - only OCR text was sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DÃ‰TAIL DU SCORE DE CONFIANCE\n",
      "============================================================\n",
      "MÃ©thode: ocr_extraction_comparison\n",
      "Score global: 87.0%\n",
      "\n",
      "ğŸ“‹ DÃ©tail par champ:\n",
      "   âœ… document_number: 100.0% (exact match)\n",
      "      Valeur extraite: 0324\n",
      "   âœ… document_date: 100.0% (Field is null)\n",
      "      Valeur extraite: None\n",
      "   âŒ supplier_name: 0.0% (not found in OCR)\n",
      "      Valeur extraite: Chan Accounting\n",
      "   âœ… customer_name: 100.0% (exact match)\n",
      "      Valeur extraite: Paul J Gordon\n",
      "   âœ… total_amount: 100.0% (exact match)\n",
      "      Valeur extraite: 500.00\n",
      "   âœ… line_items: 100.0% (average across 5 items)\n",
      "      Valeur extraite: 5 items\n",
      "\n",
      "ğŸ“Š RÃ©sumÃ©: 5 Ã©levÃ©s | 0 moyens | 1 faibles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Confidence Score Details (OCR vs Extraction)\n",
    "# =============================================================================\n",
    "\n",
    "confidence_report = results.get(\"confidence\", {})\n",
    "\n",
    "print(\"ğŸ” CONFIDENCE SCORE DETAILS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: {confidence_report.get('method', 'N/A')}\")\n",
    "print(f\"Overall score: {confidence_report.get('overall_confidence', 0):.1%}\")\n",
    "print()\n",
    "\n",
    "# Display field breakdown\n",
    "print(\"ğŸ“‹ Field breakdown:\")\n",
    "for field_name, field_data in confidence_report.get(\"fields\", {}).items():\n",
    "    conf = field_data.get(\"confidence\", 0)\n",
    "    note = field_data.get(\"note\", \"\")\n",
    "    extracted = field_data.get(\"extracted\", \"N/A\")\n",
    "    icon = \"âœ…\" if conf >= 0.9 else \"âš ï¸\" if conf >= 0.5 else \"âŒ\"\n",
    "    print(f\"   {icon} {field_name}: {conf:.1%} ({note})\")\n",
    "    print(f\"      Extracted value: {extracted}\")\n",
    "\n",
    "print()\n",
    "summary = confidence_report.get(\"summary\", {})\n",
    "print(f\"ğŸ“Š Summary: {summary.get('high_confidence', 0)} high | {summary.get('medium_confidence', 0)} medium | {summary.get('low_confidence', 0)} low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Comparaison des modes d'extraction\n",
      "\n",
      "ğŸ”§ Initializing pipeline components...\n",
      "   âœ… OCR client ready (model: mistral-document-ai-2505-2)\n",
      "   âœ… Extractor ready (model: gpt-5.1)\n",
      "   âœ… Evaluator ready (model: gpt-4o)\n",
      "   âœ… Confidence calculator ready (OCR vs Extraction comparison)\n",
      "âœ… Pipeline ready\n",
      "\n",
      "ğŸ“Š RÃ©sultats:\n",
      "\n",
      "Mode            Input Tokens    Confidence (OCR vs Extract)\n",
      "-------------------------------------------------------\n",
      "hybrid          1608            0.87\n",
      "text_only       932             1.0\n"
     ]
    }
   ],
   "source": [
    "# Comparison test: text_only vs hybrid mode\n",
    "print(\"ğŸ”¬ Extraction Mode Comparison\\n\")\n",
    "\n",
    "# Save current mode\n",
    "original_mode = config.extraction_mode\n",
    "\n",
    "# Test in text_only mode\n",
    "config.extraction_mode = \"text_only\"\n",
    "pipeline_text = DocumentPipeline(config)\n",
    "results_text = pipeline_text.process(\"invoice.png\", run_evaluation=False, verbose=False)\n",
    "\n",
    "# Restore hybrid mode\n",
    "config.extraction_mode = \"hybrid\"\n",
    "\n",
    "print(\"ğŸ“Š Results:\")\n",
    "print(f\"\\n{'Mode':<15} {'Input Tokens':<15} {'Confidence (OCR vs Extract)':<25}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "meta_hybrid = results.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "meta_text = results_text.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "\n",
    "conf_hybrid = results.get(\"confidence\", {}).get(\"overall_confidence\", \"N/A\")\n",
    "conf_text = results_text.get(\"confidence\", {}).get(\"overall_confidence\", \"N/A\")\n",
    "\n",
    "print(f\"{'hybrid':<15} {meta_hybrid.get('usage', {}).get('input_tokens', 'N/A'):<15} {conf_hybrid}\")\n",
    "print(f\"{'text_only':<15} {meta_text.get('usage', {}).get('input_tokens', 'N/A'):<15} {conf_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results exported to: output/extraction_results.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Export Results\n",
    "# =============================================================================\n",
    "\n",
    "pipeline.export_results(results, \"output/extraction_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary\n",
    "\n",
    "This notebook provides a document extraction pipeline using:\n",
    "\n",
    "1. **Mistral Document AI** for OCR\n",
    "2. **GPT-5.1 via Responses API** with strict JSON Schema validation\n",
    "3. **Confidence Calculation** by comparing OCR text with extracted values (no self-evaluation from GPT!)\n",
    "4. **Azure AI Evaluation SDK** for semantic quality assessment (Groundedness, Relevance, Coherence)\n",
    "\n",
    "### Confidence Score Method\n",
    "The confidence score is now calculated by **comparing the extracted values against the original OCR text**:\n",
    "- **Perfect match** (value found exactly in OCR) â†’ 1.0\n",
    "- **Fuzzy match** (similar text found) â†’ 0.5-0.99\n",
    "- **Not found** â†’ 0.0\n",
    "\n",
    "This is more reliable than asking GPT to self-evaluate its confidence.\n",
    "\n",
    "### To customize:\n",
    "1. Modify the schema in Section 3 for your document type\n",
    "2. Update validators in Section 7 for your rules\n",
    "3. Adjust `fuzzy_threshold` in ConfidenceCalculator (default: 0.8)\n",
    "4. Use `custom_instructions` when calling `process_document()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
