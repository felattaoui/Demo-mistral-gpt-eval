{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ” Azure Document Extraction Pipeline\n",
    "## Using Responses API with Structured Outputs\n",
    "\n",
    "### Architecture\n",
    "```\n",
    "ğŸ“„ Document (PDF/Image)\n",
    "       â”‚\n",
    "       â–¼\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Mistral Document AI     â”‚  â† OCR + Text Extraction\n",
    "â”‚  (mistral-ocr-2503)      â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼ OCR Text + Original Image\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  GPT-5.1                 â”‚  â† Structured Extraction (Responses API)\n",
    "â”‚  (Responses API +        â”‚     with strict JSON Schema\n",
    "â”‚   Structured Outputs)    â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "       â”‚\n",
    "       â–¼ Extraction Results\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚  Azure AI Evaluation SDK â”‚  â† Quality Evaluation\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1ï¸âƒ£ Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -q --upgrade openai azure-ai-evaluation azure-identity python-dotenv pydantic requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2ï¸âƒ£ Configuration\n",
    "\n",
    "Create a `.env` file in the same directory with the following variables:\n",
    "\n",
    "```env\n",
    "# Mistral Document AI\n",
    "MISTRAL_ENDPOINT=https://<your-resource>.services.ai.azure.com\n",
    "MISTRAL_API_KEY=<your-api-key>\n",
    "MISTRAL_MODEL=mistral-ocr-2503\n",
    "\n",
    "# Azure OpenAI (Responses API)\n",
    "# Found in: Foundry > Models + endpoints > Your deployment\n",
    "AZURE_OPENAI_ENDPOINT=https://<your-resource>.cognitiveservices.azure.com\n",
    "AZURE_OPENAI_API_KEY=<your-api-key>\n",
    "AZURE_OPENAI_DEPLOYMENT=gpt-5.1\n",
    "\n",
    "# Evaluation Model\n",
    "EVAL_MODEL_DEPLOYMENT=gpt-4o\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All modules imported successfully\n",
      "   - Config, Utils, OCR, Extractor, Evaluator, Confidence, Schemas, Pipeline\n"
     ]
    }
   ],
   "source": [
    "# Add src/ to path for imports\n",
    "import sys\n",
    "sys.path.insert(0, 'src')\n",
    "\n",
    "# Import from our modules\n",
    "from config import Config\n",
    "from utils import get_file_info, encode_file_to_base64, is_pdf\n",
    "from ocr import MistralOCR, create_ocr_client\n",
    "from extractor import StructuredExtractor, create_extractor\n",
    "from evaluator import QualityEvaluator, ExtractionValidator, create_evaluator\n",
    "from confidence import ConfidenceCalculator, create_confidence_calculator  # NEW: OCR vs Extraction comparison\n",
    "from schemas import DocumentExtraction, EXTRACTION_SCHEMA, get_strict_schema\n",
    "from pipeline import DocumentPipeline\n",
    "\n",
    "# Standard imports for notebook usage\n",
    "import json\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any, Optional\n",
    "\n",
    "print(\"âœ… All modules imported successfully\")\n",
    "print(\"   - Config, Utils, OCR, Extractor, Evaluator, Confidence, Schemas, Pipeline\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Configuration Status\n",
      "==================================================\n",
      "âœ… Mistral Document AI\n",
      "   Endpoint: https://felat-mgtjiioy-francecentral.cognitiveserv...\n",
      "   Model: mistral-document-ai-2505-2\n",
      "\n",
      "âœ… Azure OpenAI (Responses API)\n",
      "   Endpoint: https://doc-processing-mistral-gpt-eval.cognitives...\n",
      "   Base URL: https://doc-processing-mistral-gpt-eval.cognitiveservices.azure.com/openai/v1/\n",
      "   Deployment: gpt-5.1\n",
      "\n",
      "âœ… Evaluation: Configured\n",
      "   Deployment: gpt-4o\n",
      "==================================================\n",
      "\n",
      "ğŸ“‹ Extraction mode: hybrid\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Configuration - Load from .env file\n",
    "# =============================================================================\n",
    "\n",
    "# Load configuration from environment variables\n",
    "config = Config.from_env()\n",
    "\n",
    "# Display configuration status\n",
    "status = config.show_status()\n",
    "\n",
    "# Verify all required services are configured\n",
    "if not status[\"mistral_configured\"]:\n",
    "    print(\"\\nâš ï¸ Warning: Mistral OCR not configured - check MISTRAL_ENDPOINT and MISTRAL_API_KEY\")\n",
    "if not status[\"aoai_configured\"]:\n",
    "    print(\"\\nâš ï¸ Warning: Azure OpenAI not configured - check AZURE_OPENAI_ENDPOINT\")\n",
    "\n",
    "print(f\"\\nğŸ“‹ Extraction mode: {config.extraction_mode}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3ï¸âƒ£ Define Your Extraction Schema\n",
    "\n",
    "Customize these Pydantic models based on your document type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‹ Extraction Schema (Strict Mode for Azure OpenAI):\n",
      "   Schema name: DocumentExtraction\n",
      "   Fields: ['document_type', 'document_number', 'document_date', 'supplier', 'customer', 'total_amount', 'line_items']\n",
      "\n",
      "   ğŸ’¡ To see full schema: print(json.dumps(EXTRACTION_SCHEMA, indent=2))\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Extraction Schema - Already imported from src/schemas.py\n",
    "# =============================================================================\n",
    "\n",
    "# The schema is imported from src/schemas.py\n",
    "# You can view the Pydantic models: DocumentExtraction, LineItem, MonetaryAmount, PersonInfo\n",
    "\n",
    "print(\"ğŸ“‹ Extraction Schema (Strict Mode for Azure OpenAI):\")\n",
    "print(f\"   Schema name: DocumentExtraction\")\n",
    "print(f\"   Fields: {list(EXTRACTION_SCHEMA.get('properties', {}).keys())}\")\n",
    "print(\"\\n   ğŸ’¡ To see full schema: print(json.dumps(EXTRACTION_SCHEMA, indent=2))\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4ï¸âƒ£ Document Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document utilities ready\n",
      "   Test file: invoice.png (0.353 MB)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Document Utilities - Already imported from src/utils.py\n",
    "# =============================================================================\n",
    "\n",
    "# Functions available:\n",
    "# - encode_file_to_base64(file_path) -> (base64_string, mime_type)\n",
    "# - get_file_info(file_path) -> dict with name, extension, size\n",
    "# - is_pdf(file_path) -> bool\n",
    "\n",
    "# Test with a sample file\n",
    "test_file = \"invoice.png\"\n",
    "if Path(test_file).exists():\n",
    "    info = get_file_info(test_file)\n",
    "    print(f\"âœ… Document utilities ready\")\n",
    "    print(f\"   Test file: {info['name']} ({info['size_mb']} MB)\")\n",
    "else:\n",
    "    print(\"âœ… Document utilities ready\")\n",
    "    print(\"   â„¹ï¸ Place a document (PDF/image) to test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5ï¸âƒ£ Step 1: OCR with Mistral Document AI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Mistral OCR client initialized\n",
      "   Model: mistral-document-ai-2505-2\n",
      "   Endpoint: https://felat-mgtjiioy-francecentral.cognitiveserv...\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# OCR Client - Using src/ocr.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create OCR client from configuration\n",
    "ocr_client = create_ocr_client(config)\n",
    "\n",
    "print(f\"âœ… Mistral OCR client initialized\")\n",
    "print(f\"   Model: {config.mistral_model}\")\n",
    "print(f\"   Endpoint: {config.mistral_endpoint[:50]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6ï¸âƒ£ Step 2: Structured Extraction with GPT-5.1 (Responses API)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… StructuredExtractor initialized with Entra ID\n",
      "   Deployment: gpt-5.1\n",
      "   Extraction mode: hybrid\n",
      "   Available methods:\n",
      "     - extract(text, schema)           # Text only\n",
      "     - extract_hybrid(ocr_text, image) # OCR + Image (recommended)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Structured Extractor - Using src/extractor.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create extractor from configuration (uses Entra ID auth)\n",
    "extractor = create_extractor(config)\n",
    "\n",
    "print(f\"âœ… StructuredExtractor initialized with Entra ID\")\n",
    "print(f\"   Deployment: {config.aoai_deployment}\")\n",
    "print(f\"   Extraction mode: {config.extraction_mode}\")\n",
    "print(f\"   Available methods:\")\n",
    "print(f\"     - extract(text, schema)           # Text only\")\n",
    "print(f\"     - extract_hybrid(ocr_text, image) # OCR + Image (recommended)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7ï¸âƒ£ Step 3: Quality Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Extraction validator initialized\n",
      "   Validates: dates, amounts\n",
      "\n",
      "âœ… Confidence Calculator initialized\n",
      "   Method: OCR vs Extraction text comparison\n",
      "   No more self-evaluation from GPT!\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Extraction Validator & Confidence Calculator - Using src/evaluator.py & src/confidence.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create validator instance (dates, amounts)\n",
    "validator = ExtractionValidator()\n",
    "\n",
    "# Create confidence calculator (OCR vs Extraction comparison)\n",
    "confidence_calc = create_confidence_calculator(fuzzy_threshold=0.8)\n",
    "\n",
    "print(\"âœ… Extraction validator initialized\")\n",
    "print(\"   Validates: dates, amounts\")\n",
    "print(\"\\nâœ… Confidence Calculator initialized\")\n",
    "print(\"   Method: OCR vs Extraction text comparison\")\n",
    "print(\"   No more self-evaluation from GPT!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… QualityEvaluator initialized\n",
      "   Judge model: gpt-4o\n",
      "   Metrics: Groundedness, Relevance, Coherence\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Quality Evaluator - Using src/evaluator.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create evaluator from configuration (uses GPT-4o as judge)\n",
    "if config.eval_deployment:\n",
    "    evaluator = create_evaluator(config)\n",
    "    print(f\"âœ… QualityEvaluator initialized\")\n",
    "    print(f\"   Judge model: {config.eval_deployment}\")\n",
    "    print(f\"   Metrics: Groundedness, Relevance, Coherence\")\n",
    "else:\n",
    "    evaluator = None\n",
    "    print(\"âš ï¸ Evaluator not configured (EVAL_MODEL_DEPLOYMENT not set)\")\n",
    "    print(\"   Set EVAL_MODEL_DEPLOYMENT=gpt-4o in .env to enable evaluation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8ï¸âƒ£ Complete Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Initializing pipeline components...\n",
      "   âœ… OCR client ready (model: mistral-document-ai-2505-2)\n",
      "   âœ… Extractor ready (model: gpt-5.1)\n",
      "   âœ… Evaluator ready (model: gpt-4o)\n",
      "   âœ… Confidence calculator ready (OCR vs Extraction comparison)\n",
      "âœ… Pipeline ready\n",
      "\n",
      "\n",
      "ğŸ“ Usage:\n",
      "   results = pipeline.process(\"document.pdf\")\n",
      "   results = pipeline.process(\"document.pdf\", run_evaluation=True)\n",
      "   pipeline.display_results(results)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Complete Pipeline - Using src/pipeline.py\n",
    "# =============================================================================\n",
    "\n",
    "# Create pipeline from configuration\n",
    "pipeline = DocumentPipeline(config)\n",
    "\n",
    "print(\"\\nğŸ“ Usage:\")\n",
    "print('   results = pipeline.process(\"document.pdf\")')\n",
    "print('   results = pipeline.process(\"document.pdf\", run_evaluation=True)')\n",
    "print('   pipeline.display_results(results)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9ï¸âƒ£ Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Processing: invoice.png\n",
      "   Size: 0.353 MB\n",
      "\n",
      "ğŸ”„ Step 1: OCR with Mistral Document AI\n",
      "   âœ… OCR complete (1 pages)\n",
      "\n",
      "ğŸ”„ Step 2: Hybrid extraction with GPT-5.1 (OCR + Image)\n",
      "   âœ… Extraction complete (mode: hybrid)\n",
      "   ğŸ“Š Confidence (OCR vs Extraction): 87.0%\n",
      "\n",
      "ğŸ”„ Step 3: Quality evaluation\n",
      "   âœ… Evaluation complete (score: 77.8%)\n",
      "\n",
      "âœ… Processing complete\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Process a document\n",
    "# =============================================================================\n",
    "\n",
    "# Process the sample invoice\n",
    "results = pipeline.process(\n",
    "    file_path=\"invoice.png\",\n",
    "    run_evaluation=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ğŸ“Š EXTRACTION RESULTS\n",
      "============================================================\n",
      "\n",
      "Document Type: invoice\n",
      "Document Number: 0324\n",
      "Document Date: None\n",
      "Total Amount: 500.0 USD\n",
      "\n",
      "ğŸ“Š Confidence Score: 87.0% (method: ocr_extraction_comparison)\n",
      "   âœ… High confidence fields: 5\n",
      "   âš ï¸  Medium confidence: 0\n",
      "   âŒ Low confidence: 1\n",
      "\n",
      "Line Items (5):\n",
      "   1. Sed ut perspiciatis unde omnis iste lore... â†’ 100.0\n",
      "   2. Adipisc ing elit, sed do eius mod tempor... â†’ 100.0\n",
      "   3. Nemo enim ipsam volupt atem quia volupta... â†’ 100.0\n",
      "   4. Sed ut perspiciatis unde omnis iste lore... â†’ 100.0\n",
      "   5. Adipisc ing elit, sed do eius mod tempor... â†’ 100.0\n",
      "\n",
      "============================================================\n",
      "ğŸ“ˆ EVALUATION RESULTS\n",
      "============================================================\n",
      "   âš ï¸ Groundedness: 2.0/5\n",
      "   âœ… Relevance: 5.0/5\n",
      "   âœ… Coherence: 4.0/5\n",
      "\n",
      "Validation: 2/2 checks passed\n",
      "\n",
      "âš ï¸ OVERALL SCORE: 77.8%\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Display Results\n",
    "# =============================================================================\n",
    "\n",
    "pipeline.display_results(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” MÃ©tadonnÃ©es de l'extraction:\n",
      "   Mode: hybrid_ocr_vision\n",
      "   Input tokens: 1608\n",
      "   Output tokens: 260\n",
      "\n",
      "âœ… L'image a bien Ã©tÃ© envoyÃ©e avec le texte OCR\n"
     ]
    }
   ],
   "source": [
    "# Check extraction mode used\n",
    "metadata = results.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "print(\"ğŸ” Extraction Metadata:\")\n",
    "print(f\"   Mode: {metadata.get('mode', 'NOT DEFINED - probably text_only')}\")\n",
    "print(f\"   Input tokens: {metadata.get('usage', {}).get('input_tokens', 'N/A')}\")\n",
    "print(f\"   Output tokens: {metadata.get('usage', {}).get('output_tokens', 'N/A')}\")\n",
    "\n",
    "# If mode is not \"hybrid_ocr_vision\", the image was not sent\n",
    "if metadata.get('mode') == 'hybrid_ocr_vision':\n",
    "    print(\"\\nâœ… Image was sent along with OCR text\")\n",
    "else:\n",
    "    print(\"\\nâš ï¸ Text-only mode - only OCR text was sent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” DÃ‰TAIL DU SCORE DE CONFIANCE\n",
      "============================================================\n",
      "MÃ©thode: ocr_extraction_comparison\n",
      "Score global: 87.0%\n",
      "\n",
      "ğŸ“‹ DÃ©tail par champ:\n",
      "   âœ… document_number: 100.0% (exact match)\n",
      "      Valeur extraite: 0324\n",
      "   âœ… document_date: 100.0% (Field is null)\n",
      "      Valeur extraite: None\n",
      "   âŒ supplier_name: 0.0% (not found in OCR)\n",
      "      Valeur extraite: Chan Accounting\n",
      "   âœ… customer_name: 100.0% (exact match)\n",
      "      Valeur extraite: Paul J Gordon\n",
      "   âœ… total_amount: 100.0% (exact match)\n",
      "      Valeur extraite: 500.00\n",
      "   âœ… line_items: 100.0% (average across 5 items)\n",
      "      Valeur extraite: 5 items\n",
      "\n",
      "ğŸ“Š RÃ©sumÃ©: 5 Ã©levÃ©s | 0 moyens | 1 faibles\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Confidence Score Details (OCR vs Extraction)\n",
    "# =============================================================================\n",
    "\n",
    "confidence_report = results.get(\"confidence\", {})\n",
    "\n",
    "print(\"ğŸ” CONFIDENCE SCORE DETAILS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Method: {confidence_report.get('method', 'N/A')}\")\n",
    "print(f\"Overall score: {confidence_report.get('overall_confidence', 0):.1%}\")\n",
    "print()\n",
    "\n",
    "# Display field breakdown\n",
    "print(\"ğŸ“‹ Field breakdown:\")\n",
    "for field_name, field_data in confidence_report.get(\"fields\", {}).items():\n",
    "    conf = field_data.get(\"confidence\", 0)\n",
    "    note = field_data.get(\"note\", \"\")\n",
    "    extracted = field_data.get(\"extracted\", \"N/A\")\n",
    "    icon = \"âœ…\" if conf >= 0.9 else \"âš ï¸\" if conf >= 0.5 else \"âŒ\"\n",
    "    print(f\"   {icon} {field_name}: {conf:.1%} ({note})\")\n",
    "    print(f\"      Extracted value: {extracted}\")\n",
    "\n",
    "print()\n",
    "summary = confidence_report.get(\"summary\", {})\n",
    "print(f\"ğŸ“Š Summary: {summary.get('high_confidence', 0)} high | {summary.get('medium_confidence', 0)} medium | {summary.get('low_confidence', 0)} low\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¬ Comparaison des modes d'extraction\n",
      "\n",
      "ğŸ”§ Initializing pipeline components...\n",
      "   âœ… OCR client ready (model: mistral-document-ai-2505-2)\n",
      "   âœ… Extractor ready (model: gpt-5.1)\n",
      "   âœ… Evaluator ready (model: gpt-4o)\n",
      "   âœ… Confidence calculator ready (OCR vs Extraction comparison)\n",
      "âœ… Pipeline ready\n",
      "\n",
      "ğŸ“Š RÃ©sultats:\n",
      "\n",
      "Mode            Input Tokens    Confidence (OCR vs Extract)\n",
      "-------------------------------------------------------\n",
      "hybrid          1608            0.87\n",
      "text_only       932             1.0\n"
     ]
    }
   ],
   "source": [
    "# Comparison test: text_only vs hybrid mode\n",
    "print(\"ğŸ”¬ Extraction Mode Comparison\\n\")\n",
    "\n",
    "# Save current mode\n",
    "original_mode = config.extraction_mode\n",
    "\n",
    "# Test in text_only mode\n",
    "config.extraction_mode = \"text_only\"\n",
    "pipeline_text = DocumentPipeline(config)\n",
    "results_text = pipeline_text.process(\"invoice.png\", run_evaluation=False, verbose=False)\n",
    "\n",
    "# Restore hybrid mode\n",
    "config.extraction_mode = \"hybrid\"\n",
    "\n",
    "print(\"ğŸ“Š Results:\")\n",
    "print(f\"\\n{'Mode':<15} {'Input Tokens':<15} {'Confidence (OCR vs Extract)':<25}\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "meta_hybrid = results.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "meta_text = results_text.get(\"extraction\", {}).get(\"_metadata\", {})\n",
    "\n",
    "conf_hybrid = results.get(\"confidence\", {}).get(\"overall_confidence\", \"N/A\")\n",
    "conf_text = results_text.get(\"confidence\", {}).get(\"overall_confidence\", \"N/A\")\n",
    "\n",
    "print(f\"{'hybrid':<15} {meta_hybrid.get('usage', {}).get('input_tokens', 'N/A'):<15} {conf_hybrid}\")\n",
    "print(f\"{'text_only':<15} {meta_text.get('usage', {}).get('input_tokens', 'N/A'):<15} {conf_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Results exported to: output/extraction_results.json\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Export Results\n",
    "# =============================================================================\n",
    "\n",
    "pipeline.export_results(results, \"output/extraction_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ Summary\n",
    "\n",
    "This notebook provides a document extraction pipeline using:\n",
    "\n",
    "1. **Mistral Document AI** for OCR\n",
    "2. **GPT-5.1 via Responses API** with strict JSON Schema validation\n",
    "3. **Confidence Calculation** by comparing OCR text with extracted values (no self-evaluation from GPT!)\n",
    "4. **Azure AI Evaluation SDK** for semantic quality assessment (Groundedness, Relevance, Coherence)\n",
    "\n",
    "### Confidence Score Method\n",
    "The confidence score is now calculated by **comparing the extracted values against the original OCR text**:\n",
    "- **Perfect match** (value found exactly in OCR) â†’ 1.0\n",
    "- **Fuzzy match** (similar text found) â†’ 0.5-0.99\n",
    "- **Not found** â†’ 0.0\n",
    "\n",
    "This is more reliable than asking GPT to self-evaluate its confidence.\n",
    "\n",
    "### To customize:\n",
    "1. Modify the schema in Section 3 for your document type\n",
    "2. Update validators in Section 7 for your rules\n",
    "3. Adjust `fuzzy_threshold` in ConfidenceCalculator (default: 0.8)\n",
    "4. Use `custom_instructions` when calling `process_document()`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
